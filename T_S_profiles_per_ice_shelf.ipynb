{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "Created on Wed Mar 24 16:18 2020 (Author: Clara Burgard)\n",
    "\n",
    "This is a script to cut out the potential temperature and practical salinity in given domains near the ice-shelf front.\n",
    "\n",
    "It:\n",
    "- calculates the distance to the ice front for the small domain in front of the ice shelf\n",
    "- takes the ocean points at a given distance of the ice front and averages over them\n",
    "\n",
    "Note that these computations can be memory-heavy! I introduced several places where you can restart the computation if it crashes in the middle. These are the places where you find \"xr.open_dataset\" or \"xr.open_mfdataset\".\n",
    "\n",
    "REMEMBER to check if you really have potential temperature and practical salinity or if you need to convert them!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "import multimelt.useful_functions as uf\n",
    "import multimelt.T_S_profile_functions as tspf\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import distributed\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "READ IN THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data = # path to folder containing the file with the variable \"bathy_metry\"\n",
    "inputpath_profiles = # path to folder containing the profiles\n",
    "inputpath_isf = # path to folder containing the masks\n",
    "\n",
    "# make the domain a little smaller to make the computation even more efficient - file isf has already been made smaller at its creation\n",
    "map_lim = [-3000000,3000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "PREPARE MASK AROUND FRONT (TO RUN WITHOUT DASK!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in mask file\n",
    "file_isf_orig = xr.open_dataset(inputpath_isf+'nemo_5km_isf_masks_and_info_and_distance_new_oneFRIS.nc')\n",
    "# Remove very small ice shelves not represented in you resolution\n",
    "nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "# Remove quite small ice shelves (depending on your effective resolution), we choose to remove everything below an area of 2500km2\n",
    "large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "file_isf = file_isf_nonnan.sel(Nisf=large_isf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read longitude and latitude\n",
    "lon = file_isf['longitude']\n",
    "lat = file_isf['latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in bathymetry to define continental shelf\n",
    "file_mask_orig = # path to file containing variable \"bathy_metry\"\n",
    "file_mask = uf.cut_domain_stereo(file_mask_orig, map_lim, map_lim).squeeze().drop('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in just one yearly file to compute spatial characteristics\n",
    "T_S_ocean_oneyear = xr.open_dataset(inputpath_profiles+'T_S_theta_ocean_corrected_2000.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ocean points not covered by ice shelves\n",
    "ocean = np.isfinite(T_S_ocean_oneyear['theta_ocean'].isel(time=0,depth=0)).drop('time').drop('depth')\n",
    "# only points below 1500 m\n",
    "offshore = file_mask['bathy_metry'] > 1500 # .drop('lon').drop('lat')\n",
    "# only points above 1500 m\n",
    "contshelf = file_mask['bathy_metry'] <= 1500 # .drop('lon').drop('lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_domains = (ocean & contshelf).load() #<= checked if it does what it should and it does! :)\n",
    "#mask_domains = (ocean).load()\n",
    "\n",
    "# define one domain on the continental shelf and one domain \"offshore\"\n",
    "mask_domains = xr.DataArray([(ocean & contshelf), (ocean & offshore)],\n",
    "                            dims={'profile_domain': ['close_cont_shelf','offshore'], 'y': contshelf.y, 'x': contshelf.x}).load()\n",
    "\n",
    "# define lon_box and lat_box where to compute the distance to the ice-shelf front\n",
    "lon_box = xr.DataArray(np.array([10.0, 10.0]), coords=[('profile_domain', ['close_cont_shelf','offshore'])])\n",
    "lat_box = xr.DataArray(np.array([3.5, 3.5]), coords=[('profile_domain', ['close_cont_shelf','offshore'])])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare masks around the ice shelf front over which we want to take the mean profiles.\n",
    "close_region_around_isf_mask = tspf.mask_boxes_around_IF_new(lon, lat, mask_domains, \n",
    "                                    file_isf['front_min_lon'], file_isf['front_max_lon'], \n",
    "                                    file_isf['front_min_lat'], file_isf['front_max_lat'],  \n",
    "                                    lon_box, lat_box, \n",
    "                                    file_isf['isf_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distance of ocean points to the ice-shelf front in the masked region on the continental shelf (for each ice shelf) and write to netcdf\n",
    "dist_list = [ ]\n",
    "for kisf in tqdm(file_isf['Nisf']):\n",
    "        \n",
    "        if (file_isf['IF_mask']==kisf).sum() > 0:\n",
    "            region_to_cut_out = close_region_around_isf_mask.sel(profile_domain='close_cont_shelf').sel(Nisf=kisf)\n",
    "            region_to_cut_out = region_to_cut_out.where(region_to_cut_out > 0, drop=True)\n",
    "            IF_region = file_isf['IF_mask'].where(file_isf['IF_mask']==kisf, drop=True)\n",
    "\n",
    "            dist_from_front = tspf.distance_isf_points_from_line_small_domain(region_to_cut_out,IF_region)\n",
    "            dist_list.append(dist_from_front)\n",
    "    \n",
    "dist_all = xr.concat(dist_list, dim='Nisf').reindex_like(file_isf)\n",
    "dist_all.to_dataset(name='dist_from_front').to_netcdf(inputpath_profiles+'dist_to_ice_front_only_contshelf_oneFRIS.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the offshore mask is ready already and can be written to netcdf directly - no need for distance from the grounding line, it is just a box offshore of the ice shelf\n",
    "close_region_around_isf_mask.sel(profile_domain='offshore').to_dataset(name='mask').to_netcdf(inputpath_profiles+'mask_offshore_oneFRIS.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "COMPUTING THE MEAN PROFILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "DEPENDING ON THE SIZE OF YOUR DATA, THIS WILL REQUIRE USING DASK\n",
    "\n",
    "For example, open a client like this: (check that the number of workers is lower or equal to the number of cores you use and the memory limit is equal to the memory of your cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "From experience it makes sense to restart your kernel and restart here to erase memory used by previous operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client(n_workers=16, dashboard_address=':8795', local_directory='/tmp', memory_limit='4GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "CONTINENTAL SHELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the different domains on the continental shelf\n",
    "bbox_da = xr.DataArray(np.array([10000., 25000., 50000., 100000.]), coords=[('dist_from_front', [10,25,50,100])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "If workers don't die: with 12 cores, took approx 1hour. If workers die, divide work by years (all_in_one = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one = False # False if worker die if you put in all years at once, True if workers don't die \n",
    "# about chunking: the values here are the most \"efficient\" ones for me here, might require adjusting if your workers have more/less memory available\n",
    "if all_in_one:\n",
    "    dist_to_front_file = xr.open_mfdataset(inputpath_profiles+'dist_to_ice_front_only_contshelf_oneFRIS.nc',chunks={'x': 50, 'y': 50})\n",
    "    T_S_ocean_files = xr.open_mfdataset(inputpath_profiles+'T_S_theta_ocean_corrected_*.nc', concat_dim='time', chunks={'x': 50, 'y': 50, 'depth': 50}, parallel=True)\n",
    "    T_S_ocean_oneyear = xr.open_mfdataset(inputpath_profiles+'T_S_theta_ocean_corrected_2000.nc',chunks={'x': 50, 'y': 50, 'depth': 50})\n",
    "else:\n",
    "    dist_to_front_file = xr.open_mfdataset(inputpath_profiles+'dist_to_ice_front_only_contshelf_oneFRIS.nc',chunks={'x': 100, 'y': 100})\n",
    "    T_S_ocean_files = xr.open_mfdataset(inputpath_profiles+'T_S_theta_ocean_corrected_*.nc', concat_dim='time', chunks={'x': 100, 'y': 100, 'depth': 50}, parallel=True) \n",
    "    T_S_ocean_oneyear = xr.open_mfdataset(inputpath_profiles+'T_S_theta_ocean_corrected_2000.nc',chunks={'x': 100, 'y': 100, 'depth': 50}) #\n",
    "dist_to_front = dist_to_front_file['dist_from_front']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "MAKING THE MEAN DIRECTLY NEEDS TO MUCH MEMORY \n",
    "\n",
    "so we divide the steps to make the mean: (1) the sum of T and S over the domain of interest, (2) the sum of the points of interest, (3) dividing (1) by (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Prepare sum (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask of the distance domain we want\n",
    "mask_km = dist_to_front <= bbox_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over all T and S in that region\n",
    "ds_sum = (T_S_ocean_files * mask_km).sum(['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check format\n",
    "ds_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this sum to netcdf (this will start the stuff in dask)\n",
    "if all_in_one:\n",
    "    ds_sum = ds_sum.load()\n",
    "    ds_sum.to_netcdf(inputpath_profiles+'ds_sum_for_mean_contshelf.nc')\n",
    "else:\n",
    "    yearly_datasets = list(tspf.split_by_chunks(ds_sum.unify_chunks(),'time'))\n",
    "    paths = [tspf.create_filepath(ds, 'ds_sum_for_mean_contshelf', inputpath_profiles, ds.time[0].values) for ds in yearly_datasets]\n",
    "    xr.save_mfdataset(datasets=yearly_datasets, paths=paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Prepare number of points by which you divide: sum (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_in_one:\n",
    "    ds_sum = xr.open_mfdataset(inputpath_profiles+'ds_sum_for_mean_contshelf.nc')\n",
    "else:\n",
    "    ds_sum = xr.open_mfdataset(inputpath_profiles+'ds_sum_for_mean_contshelf*.nc', concat_dim='time', parallel=True).drop('profile_domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask points in domain of interest in depth where there is water\n",
    "mask_depth = T_S_ocean_oneyear['salinity_ocean'].squeeze().drop('time') > 0\n",
    "mask_all = mask_km & mask_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum all points in domain of interest in depth where there is water, keep depth dimension\n",
    "mask_sum = mask_all.sum(['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the tasks in dask\n",
    "mask_sum = mask_sum.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Make the mean (3) - divide (1) by (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mean = ds_sum/mask_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mean = ds_mean.drop('profile_domain').rename({'dist_from_front': 'profile_domain'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to netcdf\n",
    "ds_mean.to_netcdf(inputpath_profiles+'T_S_mean_prof_corrected_km_contshelf_1980-2018.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "OFFSHORE PROFILES\n",
    "\n",
    "Same procedure as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_S_ocean_files = xr.open_mfdataset(inputpath_profiles+'T_S_theta_ocean_corrected_*.nc', concat_dim='time', chunks={'x': 1000, 'y': 1000, 'depth': 50}, parallel=True)\n",
    "T_S_ocean_oneyear = xr.open_mfdataset(inputpath_profiles+'T_S_theta_ocean_corrected_2000.nc',chunks={'x': 1000, 'y': 1000, 'depth': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_offshore_file = xr.open_mfdataset(inputpath_profiles+'mask_offshore_oneFRIS.nc',chunks={'x': 1000, 'y': 1000}).sel(Nisf=[11])\n",
    "mask_offshore = mask_offshore_file['mask'].drop('profile_domain')\n",
    "mask_depth = T_S_ocean_oneyear['salinity_ocean'].squeeze().drop('time') > 0\n",
    "mask_all_offshore = mask_offshore & mask_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Sum (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sum_offshore = (T_S_ocean_files * mask_offshore).sum(['x','y'])\n",
    "ds_sum_offshore['profile_domain'] = np.array([1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sum_offshore = ds_sum_offshore.load()\n",
    "ds_sum_offshore.to_netcdf(inputpath_profiles+'ds_sum_for_mean_offshore.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Sum (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sum_offshore = mask_all_offshore.sum(['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sum_offshore = mask_sum_offshore.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "(3) Divide (1) by (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mean_offshore = ds_sum_offshore/mask_sum_offshore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mean_offshore.to_netcdf(inputpath_profiles+'T_S_mean_prof_corrected_km_offshore_1980-2018.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "COMBINE BOTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mean_offshore = xr.open_dataset(inputpath_profiles+'T_S_mean_prof_corrected_km_offshore_1980-2018.nc')\n",
    "ds_mean = xr.open_dataset(inputpath_profiles+'T_S_mean_prof_corrected_km_contshelf_1980-2018.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mean_both = xr.concat([ds_mean, ds_mean_offshore], dim='profile_domain')\n",
    "ds_mean_both.to_netcdf(inputpath_profiles+'T_S_mean_prof_corrected_km_contshelf_and_offshore_1980-2018.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ice_shelf_mask",
   "language": "python",
   "name": "ice_shelf_mask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
