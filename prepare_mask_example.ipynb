{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Created on Wed Dec 15 13:54 2021 (Author: Clara Burgard)\n",
    "\n",
    "This is an example script to prepare the mask file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import multimelt.plume_functions as pf\n",
    "import multimelt.box_functions as bf\n",
    "import multimelt.create_isf_mask_functions as isfmf\n",
    "import multimelt.deepmelt_functions as dmf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_metadata = './multimelt/mask_info/'\n",
    "outputpath_mask = # path where you want to store your mask netcdf file\n",
    "outputpath_boxes = # path where you want to store the box characteristics files\n",
    "outputpath_plumes = # path where you want to store the plumes characteristics files\n",
    "outputpath_deepmelt = # path where you want to store the plumes characteristics files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bed_orig = # xr.DataArray containing the bathymetry (on grid EPSG:3031)\n",
    "file_draft = # xr.DataArray containing the actual ice draft depth (not smoothed out through a grid cell mean when the ice concentration is <1)\n",
    "file_msk = # xr.DataArray containing mask: 0 = ocean, 1 = ice shelves, 2 = grounded ice (on grid EPSG:3031)\n",
    "file_isf_conc = # xr.DataArray containing the ice shelf concentration in each grid cell\n",
    "\n",
    "xx = file_msk['x']\n",
    "yy = file_msk['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Create the masks for ice shelves/ground/pinning points/grounding line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "OPTION 1: based on manually set limits (thank you Nico Jourdain!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_ds = isfmf.create_mask_and_metadata_isf(\n",
    "                    file_msk, # file containing info about the grid (needs to be a domain centered around the South Pole!)                                       \n",
    "                    -1*file_bed_orig, # negative bathymetry           \n",
    "                    file_msk, # original mask\n",
    "                    -1*file_draft, # negative ice draft depth\n",
    "                    file_isf_conc, # ice shelf concentration\n",
    "                    False, # not chunked (CAREFUL! chunks not necessarily supported yet)\n",
    "                    inputpath_metadata+'lonlat_masks.txt', # lon/lat boundaries of the ice shelves\n",
    "                    outputpath_mask, # output path for output to write out intermediate steps\n",
    "                    inputpath_metadata + 'iceshelves_metadata_Nico.txt', # file containing name and Rignot data about the different ice shelves\n",
    "                    inputpath_metadata+'GL_flux_rignot13.csv', # file containing the flux at the grounding line from Rignot et al 2013\n",
    "                    ground_point ='no', # if 'yes', the grounding line is defined on the ice shelf points at the border to the ground\n",
    "                    FRIS_one=True, # do you want to count Filchner-Ronne as one ice shelf? True if yes, False if you want to have them as two separate ice shelves\n",
    "                    mouginot_basins=False, # beta-tested the definition of ice shelves with drainage basins from J. Mouginot, stopped working on it so do not count on it\n",
    "                    variable_geometry=False, # TO BE USED FOR GEOMETRIES DIFFERENT FROM PRESENT - if True, the ice shelves havee a slightly different geometry than present and the limits have to be changed\n",
    "                    write_ismask = 'yes', write_groundmask = 'yes', write_outfile='yes', # if you already wrote one of these files, you can set option to 'no'\n",
    "                    dist=40, # Defines the size of the starting square for the ground mask - should be small if the resolution is coarse and high if the resolution is fine - can be modulated\n",
    "                    add_fac=120, # Defines additional iterations for the propagation for the ground mask - can be modulated\n",
    "                    connectivity=4, # if variable_geometry = True:if 8 it looks at all 8 directions to see define neighbouring ice shelf points, if 4 only horizontally and vertically\n",
    "                    threshold=4, # if variable_geometry = True: an entity of 4 points is considered as one ice shelf\n",
    "                    write_metadata = 'yes', # writes out the file with only metadata\n",
    "                    AlexIslandisf = [54,75] # the IDs of the ice shelves situated on Alexander Island and therefore have a grounding line not on \"mainland Antarctica\"\n",
    "                    ) \n",
    "\n",
    "# Write to netcdf\n",
    "print('------- WRITE TO NETCDF -----------')\n",
    "whole_ds.to_netcdf(outputpath_mask + 'mask_file.nc','w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "OPTION 2: based on IMBIE2 basins (but you need a file interpolated to your own grid spacing!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_IMBIE = xr.open_dataset(inputpath_IMBIE + 'Mask_Iceshelf_IMBIE2_v2_5km.nc') # Note that this file should be interpolated to the target grid spacing you want!\n",
    "mask_IMBIE_containing_names = xr.open_dataset(inputpath_IMBIE + 'Mask_Iceshelf_IMBIE2_v2.nc')\n",
    "mask_IMBIE_containing_names_corrected = xr.concat([mask_IMBIE_containing_names['NAME'].drop_sel(ID=1),xr.DataArray(data=['Jelbart'], dims=['ID']).assign_coords({'ID': [134]})], dim='ID') # because 1 will be ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whole_ds = isfmf.create_mask_and_metadata_isf(\n",
    "                    file_msk, # file containing info about the grid (needs to be a domain centered around the South Pole!)                                       \n",
    "                    -1*file_bed_orig, # negative bathymetry           \n",
    "                    file_msk, # original mask\n",
    "                    -1*file_draft, # negative ice draft depth\n",
    "                    file_isf_conc, # ice shelf concentration\n",
    "                    False, # not chunked (CAREFUL! chunks not necessarily supported yet)\n",
    "                    inputpath_metadata + 'Mask_Iceshelf_IMBIE2_v2.nc', # Note that this file should be interpolated to the target grid spacing you want! Also note that the ID for Ross, FRIS, and Jelbart are hard-coded in the function. So make sure you use this IMBIE2 file\n",
    "                    outputpath_mask, # output path for output to write out intermediate steps\n",
    "                    mask_IMBIE_containing_names_corrected, # contains the names associated to the IDs\n",
    "                    inputpath_metadata+'GL_flux_rignot13.csv', # file containing the flux at the grounding line from Rignot et al 2013\n",
    "                    mouginot_basins=True, # needs to be True to use the IMBIE2 file in the right way\n",
    "                    variable_geometry=False, # TO BE USED FOR GEOMETRIES DIFFERENT FROM PRESENT - if True, the ice shelves havee a slightly different geometry than present and the limits have to be changed\n",
    "                    write_ismask = 'yes', write_groundmask = 'yes', write_outfile='yes', # if you already wrote one of these files, you can set option to 'no'\n",
    "                    ground_point ='no', # if 'yes', the grounding line is defined on the ice shelf points at the border to the ground\n",
    "                    FRIS_one=True, # do you want to count Filchner-Ronne as one ice shelf? True if yes, False if you want to have them as two separate ice shelves\n",
    "                    dist=40, # Defines the size of the starting square for the ground mask - should be small if the resolution is coarse and high if the resolution is fine - can be modulated\n",
    "                    add_fac=250, # Defines additional iterations for the propagation for the ground mask - can be modulated\n",
    "                    connectivity=4,  # if variable_geometry = True:if 8 it looks at all 8 directions to see define neighbouring ice shelf points, if 4 only horizontally and vertically\n",
    "                    threshold=4, # if variable_geometry = True: an entity of 4 points is considered as one ice shelf\n",
    "                    write_metadata='yes', # writes out the file with only metadata\n",
    "                    AlexIslandisf = [101,102,103,105,106,107,109] # the IDs of the ice shelves situated on Alexander Island and therefore have a grounding line not on \"mainland Antarctica\"\n",
    "                    ) \n",
    "\n",
    "# Write to netcdf\n",
    "print('------- WRITE TO NETCDF -----------')\n",
    "whole_ds.to_netcdf(outputpath_mask + 'mask_file.nc','w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Prepare the box characteristics (writes the output directly to files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_ds = xr.open_dataset(outputpath_mask + 'mask_file.nc')\n",
    "\n",
    "nonnan_Nisf = whole_ds['Nisf'].where(np.isfinite(whole_ds['front_bot_depth_max']), drop=True).astype(int)\n",
    "file_isf_nonnan = whole_ds.sel(Nisf=nonnan_Nisf)\n",
    "large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True) # only look at ice shelves with area larger than 2500 km2 - if you want to keep to largest ones\n",
    "file_isf = file_isf_nonnan.sel(Nisf=large_isf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "isf_var_of_int = file_isf[['ISF_mask', 'GL_mask', 'dGL', 'dIF', 'latitude', 'longitude', 'isf_name']]\n",
    "out_2D, out_1D = bf.box_charac_file(file_isf['Nisf'],isf_var_of_int, -1*file_draft, file_isf_conc, outputpath_boxes, max_nb_box=10)\n",
    "\n",
    "print('------ WRITE TO NETCDF -------')\n",
    "out_2D.to_netcdf(outputpath_boxes + 'boxes_2D.nc', 'w')\n",
    "out_1D.to_netcdf(outputpath_boxes + 'boxes_1D.nc', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Prepare the plume characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plume_param_options = ['cavity','new_lazero', 'local'] #'lazero' has been replaced by 'new_lazero' => 'new_lazero' propagates each grounding-line point depth across possible plume paths. 'lazero' looks, at each point, into 16 directions (like in Lazeroms et al. 2018)\n",
    "\n",
    "plume_var_of_int = file_isf[['ISF_mask', 'GL_mask', 'IF_mask', 'dIF', 'dGL_dIF', 'latitude', 'longitude', 'front_ice_depth_avg']]\n",
    "\n",
    "# Compute the ice draft\n",
    "ice_draft_pos = file_draft\n",
    "ice_draft_neg = -1*ice_draft_pos\n",
    "\n",
    "plume_charac = pf.prepare_plume_charac(plume_param_options, ice_draft_pos, plume_var_of_int,\n",
    "                                      grad_corr=0, dir_nb=16, extra_shift=2, dist_incl=0) # these options are for the new_lazero option, check function and code for more info\n",
    "\n",
    "print('------ WRITE TO NETCDF -------')\n",
    "plume_charac.to_netcdf(outputpath_plumes+'plume_characteristics.nc', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9bf1a-3ea9-423d-bacd-4042b49a8ecb",
   "metadata": {},
   "source": [
    "Prepare characteristics needed for DeepMelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8700ea-c860-4509-bf85-54dc07a2435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here bed should be positive and draft should be positive\n",
    "file_bed = file_bed_orig # double_check that it is positive\n",
    "file_draft = # double_check that it is positive\n",
    "\n",
    "bb_merid_slope = None\n",
    "bb_zonal_slope = None\n",
    "\n",
    "ice_merid_slope = None\n",
    "ice_zonal_slope = None\n",
    "\n",
    "dx = file_isf.x[2] - file_isf.x[1]\n",
    "dy = file_isf.y[2] - file_isf.y[1]\n",
    "\n",
    "for kisf in tqdm(file_isf.Nisf):\n",
    "    #print(kisf.values)\n",
    "    bb_lonslope, bb_latslope = dmf.slope_zonal_merid(kisf, file_isf, -1*file_bed, dx, dy)\n",
    "    #print('here1')\n",
    "    ice_lonslope, ice_latslope = dmf.slope_zonal_merid(kisf, file_isf, -1*file_draft, dx, dy)\n",
    "    #print('here2')\n",
    "    if bb_merid_slope is None:\n",
    "        bb_merid_slope = bb_lonslope\n",
    "        bb_zonal_slope = bb_latslope\n",
    "        ice_merid_slope = ice_lonslope\n",
    "        ice_zonal_slope = ice_latslope\n",
    "    else:\n",
    "        bb_merid_slope = bb_merid_slope.combine_first(bb_lonslope)\n",
    "        bb_zonal_slope = bb_zonal_slope.combine_first(bb_latslope)\n",
    "        ice_merid_slope = ice_merid_slope.combine_first(ice_lonslope)\n",
    "        ice_zonal_slope = ice_zonal_slope.combine_first(ice_latslope)\n",
    "\n",
    "bb_merid_slope_smooth = bb_merid_slope.reindex_like(file_isf)\n",
    "bb_zonal_slope_smooth = bb_zonal_slope.reindex_like(file_isf)\n",
    "ice_merid_slope_smooth = ice_merid_slope.reindex_like(file_isf)\n",
    "ice_zonal_slope_smooth = ice_zonal_slope.reindex_like(file_isf)\n",
    "\n",
    "\n",
    "dataset_res_merid_bed = bb_merid_slope_smooth.to_dataset(name='slope_bed_lon')\n",
    "dataset_res_zonal_bed = bb_zonal_slope_smooth.to_dataset(name='slope_bed_lat')\n",
    "dataset_res_merid_ice = ice_merid_slope_smooth.to_dataset(name='slope_ice_lon')\n",
    "dataset_res_zonal_ice = ice_zonal_slope_smooth.to_dataset(name='slope_ice_lat')\n",
    "\n",
    "\n",
    "dataset_res = xr.merge([dataset_res_merid_bed,dataset_res_zonal_bed,dataset_res_merid_ice,dataset_res_zonal_ice,file_bed.rename('bathymetry')])\n",
    "dataset_res.to_netcdf(outputpath_deepmelt+'deepmelt_slope_info_bedrock_draft_latlon.nc','w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
